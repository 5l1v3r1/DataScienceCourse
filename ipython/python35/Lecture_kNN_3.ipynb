{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "<p>KNN is a form of <i>instance</i>, or <i>memory</i> based learning wherein we don't learn a function $f(X)$ to estimate $E[Y|X]$. Rather, to make a classification for a given instance $X_i$, we search the training data for the $k$ nearest neighbors, as defined by some distance metric $d(X_i,X_j)$. The estimate of $E[Y|X]$ is then given by:<br><br>\n",
    "\n",
    "<center>$E[Y|X] = \\sum\\limits_{y_i \\in N(X)}^k Y_i$</center><br><br>\n",
    "\n",
    "The most common distance function used in kNN is the <i>Euclidean Distance</i>.<br><br>\n",
    "\n",
    "Let $X = <x^1,...x^p>$ be a $p$-dimensional vector, then for two instances $i$ and $j$:<br><br>\n",
    "<center>$eud(X_i,X_j) = \\sqrt{(x_i^1-x_j^1)^2+...+(x_i^p-x_j^p)^2} = \\sqrt{\\sum\\limits_{t=1}^p (x_i^t-x_j^t)^2}$\n",
    "</center>\n",
    "<br><br>\n",
    "In the following example we run kNN on some real data from an ad conversion prediction dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This block loads the data, downsamples it, splits to train/test and rescales it.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/kevin/Documents/GitHub/DS_course/ipython\")\n",
    "import numpy as np\n",
    "import course_utils as bd\n",
    "import imp\n",
    "imp.reload(bd)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#KNN is implemented as a class in sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Load data and downsample for a 50/50 split, then split into a train/test\n",
    "f = 'C:/Users/kevin/Documents/GitHub/DS_course/datasets/ads_dataset_cut.txt'\n",
    "\n",
    "train_split = 0.5\n",
    "\n",
    "tdat = pd.read_csv(f,header=0,sep='\\t')\n",
    "moddat = bd.evenSplit(tdat,'y_buy')\n",
    "moddat_scale =bd.scaleData(moddat)\n",
    "\n",
    "#We know the dataset is sorted so we can just split by index\n",
    "train = moddat[:int(np.floor(moddat.shape[0]*train_split))]\n",
    "test = moddat[int(np.floor(moddat.shape[0]*train_split)):]\n",
    "\n",
    "#Scale the data to [0,1] interval...note that scaleData doesn't rescale by variance\n",
    "train_scale = moddat_scale[:int(np.floor(moddat_scale.shape[0]*train_split))]\n",
    "test_scale = moddat_scale[int(np.floor(moddat_scale.shape[0]*train_split)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def testKnn(k,train,test,lab,p=2):\n",
    "    '''\n",
    "    Run kNN classification and return accuracy \n",
    "    '''\n",
    "    X_train=train.drop(lab,1)\n",
    "    y_train=train[lab]\n",
    "    #Specify k and use p=2 for Euclidean distance\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k,p=p)\n",
    "    #doesn't really fit the model in a classic sense, it does some intelligent partitioning of the data\n",
    "    neigh.fit(X_train,y_train)\n",
    "    #Get the predictions...note, this returns a class label prediction, not a probability\n",
    "    cm=confusion_matrix(neigh.predict(test.drop(lab,1)),test[lab])\n",
    "    return (cm[0][0]+cm[1][1])/float(np.sum(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Given a feature set and a distance function, the paramater that controls complexity is the number of neighbors used $k$. Like with all complexity parameters, the optimal value is dependent on the data, and is generally the one that exploits the best bias-variance tradeoff. <br><br>\n",
    "\n",
    "The following is an example grid search to find the optimal $k$. We also create 2 other variants of the data to show the effect of each on accuracy.<br>\n",
    "\n",
    "<ul>\n",
    "    <li>The training set size</li>\n",
    "    <li>The scaling of the features</li>\n",
    "</ul><br>\n",
    "\n",
    "Note that Euclidean distance is not scale invariant. Features with higher norms will in general dominate the neighborhood. If the features with the highest norms are also not that predictive, then the model predictions suffer. It is often best to rescale the features before running kNN.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "go=1\n",
    "if (go==1):\n",
    "    s1=[ testKnn(i,train[:50],test,'y_buy',p=2) for i in range(1,40) ]\n",
    "    s2=[ testKnn(i,train,test,'y_buy',p=2) for i in range(1,40) ]\n",
    "    s3=[ testKnn(i,train_scale,test_scale,'y_buy',p=2) for i in range(1,40) ]\n",
    "    \n",
    "x = range(1,40)\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "p1 = plt.plot(x,s1,'r-',label='Unscaled, 50 Training Examples')\n",
    "p2 = plt.plot(x,s2,'b-',label='Unscaled, Full Training Set')\n",
    "p3 = plt.plot(x,s3,'g-',label='Scaled, Full Training Set')\n",
    "plt.title('Accuracy of kNN with varied k and other options')\n",
    "plt.legend(loc=3)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Test Set Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>One major challenge with kNN is that the expected distance between any two points in $p$-dimensional space increases as $p$ increases. The following is a simulation that illustrates this concept.<br><br>\n",
    "\n",
    "We generate $n$ uniformly distributed samples in a $p$-dimensional hyper-cube. We then show the average distance between instances increases as we consider more dimensions.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Show curse of dimensionality\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "#Generate a random matrix in a uniform hyper-cube\n",
    "\n",
    "n = 100\n",
    "d = 50\n",
    "X = np.random.rand(n, d)\n",
    "\n",
    "dims = range(2,d)\n",
    "dists=dict()\n",
    "\n",
    "go = 1\n",
    "if (go==1):\n",
    "    for dim in dims:\n",
    "        dists[dim]=list()\n",
    "\n",
    "    #Now get all pairwise distances of various dimensions\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            for dim in dims:\n",
    "                dists[dim].append(np.sqrt((sum(X[i,:dim]-X[j,:dim])**2)))\n",
    "\n",
    "    means=[ np.array(dists[dim]).mean() for dim in dims ]\n",
    "\n",
    "\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(dims,means)\n",
    "plt.title('Avg Pairwise Distance in Unit Hypercube by Dimension')\n",
    "plt.legend(loc=3)\n",
    "ax.set_xlabel('Dimension of Cube')\n",
    "ax.set_ylabel('Avg Pairwise Distance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we plot this histogram of distances, and not just the mean distance by dimension $p$.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Plot histogram of pairwise distances\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r = (0,12)\n",
    "b = 24\n",
    "fig = plt.figure()\n",
    "frame = plt.gca()\n",
    "ax = fig.add_subplot(111)\n",
    "h1 = plt.hist(dists[2], range=r, bins=b, normed=True, histtype='step',stacked=True,label='Dim=2')\n",
    "h2 = plt.hist(dists[10],range=r, bins=b, normed=True, histtype='step',stacked=True,label='Dim=10')\n",
    "h3 = plt.hist(dists[20],range=r, bins=b, normed=True, histtype='step',stacked=True,label='Dim=30')\n",
    "h4 = plt.hist(dists[40],range=r, bins=b, normed=True, histtype='step',stacked=True,label='Dim=40')\n",
    "frame.axes.get_yaxis().set_ticks([])\n",
    "ax.set_xlabel('Pairwise Distance')\n",
    "ax.set_ylabel('Pct of Pairs with Distance')\n",
    "plt.title('Histogram of Pairwise Distances by Dimensionality of X')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
